{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61e04b9f-fbea-4b3b-8947-7dee54a8b938",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "Contains notes on study of various optimisation algorithms for training deep neural networks.\n",
    "Not intended as a learning resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81da6a87-ac93-4fa2-b8ec-fc658074a773",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693f60d9-954c-44ec-b384-0e9601a138be",
   "metadata": {},
   "source": [
    "### Some notation:\n",
    "- $\\Theta$: The parameter space of a machine learning model.\n",
    "- $f_{\\theta}(\\cdot)$ - The model to train. Given a feature vector, it predicts a target variable (see below)\n",
    "- $\\mathcal{D}$: A dataset for supervised learning, formally a set of $n$ pairs, $(X_i,y_i)$, where $X_i$ denotes a feature vector (input variable), and $y_i$ is the corresponding target variable.\n",
    "- $\\widehat{y}_i$ - The model prediction given a feature vector, $X_i$.\n",
    "- $L(y_i,\\widehat{y_i})$ - The loss function (essentially a distance function between actual target variable and that predicted by model.\n",
    "- $C$ - The cost function, which measures a model's error over a group of training instances ($C = \\sum_{i} L(y_i,\\widehat{y}_i)$).\n",
    "- $\\nabla_{\\theta_{t}}C$ - The derivative of the cost function w.r.t the model parameters at time step $t$. A vector.\n",
    "\n",
    "**What is an optimiser?** - Our ultimate aim in training a neural network is to find the optimal parameterised function $f_{\\theta}$, for a given task. This is reduces to searching over the parameter space for $\\theta$ for an optimal set of parameters, $\\theta_{opt}$. Given that the parameter space can be millions or billions of dimensions in current neural networks, this search must be performed intelligently. This is the job of our optimisation algorithm. \n",
    "\n",
    "### Vanilla gradient descent (VGD)\n",
    "\n",
    "There are essentially three variants of gradient descent. They are characterised by the number of samples in the training dataset that are used to compute the cost function (which is in turn needed for the gradient vector, and paramater updates). They are as follows:\n",
    "\n",
    "1. **Batch gradient descent**: Uses *all* the training instances in the training dataset to compute the cost function. Whilst this gives the best possible reflection of model performance, it is expensive to implement for large training datasets. \n",
    "2. **Stochastic gradient descent**: The converse of batch gradient descent, stochastic gradient descent uses just *one* training instance to calculate the cost, with this training instance chosen randomly. Whilst cheap to implement, this method leads to very noisy parameter updates (movement over parameter space follows no clear direction over short timescales), due to the inaccuracy rising from modelling model performance across the whole training dataset with its loss on just one training instance.\n",
    "3. **Mini-batch gradient descent**: An attempt at achieving the best of both worlds with regards to batch/stochastic gradient descent. One chooses a batch size, $m$, and the cost is computed as the average over $m$ randomly chosen training instances.\n",
    "\n",
    "<center>\n",
    "    <figure>\n",
    "        <img src='Images/vanilla_grad_descent.png' width=400 alt='missing' />\n",
    "        <figcaption>Src: https://towardsdatascience.com/improving-vanilla-gradient-descent-f9d91031ab1d</figcaption>\n",
    "    </figure>\n",
    "</center>\n",
    "\n",
    "\n",
    "However, the following challenges are present in all versions of vanilla gradient descent:\n",
    "1. **Difficulty in choosing learning rate** - A large learning rate may lead to no convergence, with the optimiser instead just oscillating around some (global/local) minimum. Too small a learning rate runs the risk of getting stuck in local minima, as well as extremely slow convergence (and therefore large training times). \n",
    "2. **Non-adaptive learning rate schedules** - One quick fix to problem (1) is to have the learning rate vary as a function of time, or epoch number. This is known as setting a learning rate schedule. Whilst this sometimes works, the learning rate must be set in advance of the training process, meaning they cannot adapt to the datasets characteristics (for example, it would be desirable to have a learning rate that adapts to the magnitude of the gradient).\n",
    "3.) **Global learning rates** - In the methods given above, one learning rate is applied to all parameters. We would like a learning rate that accounts for this.\n",
    "4.) **Getting stuck in local minima** - For highly non-convex functions, it is likely that variants of gradient descent will get stuck in suboptimal local minima. We would like to develop techniques that encourage convergence toward global minima in parameter space.\n",
    "\n",
    "\n",
    "### Momentum:\n",
    "\n",
    "As hinted above, some directions in parameter space may be considerably steeper than others. This can lead to oscillation without real progress towards local/global minimum when using VGD. The momentum adaptation to gradient descent gives the parameter update some inertia in the direction that it is moving, similar to that which a heavy ball has when rolling down a slope. This is shown in the figure below:\n",
    "\n",
    "<center>\n",
    "    <figure>\n",
    "        <img src='Images/momentum_img.png' width=600 alt='missing' />\n",
    "        <figcaption></figcaption>\n",
    "    </figure>\n",
    "</center>\n",
    "\n",
    "Formally, we can define the weight update for gradient descent with a momentum adaptation as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{t+1} = \\theta_{t} + \\Delta \\theta_{t} \\\\\n",
    "\\Delta\\theta_t = -\\eta \\nabla_{\\theta_t}C + \\gamma \\Delta \\theta_{t-1}\n",
    "\\end{align*}\n",
    "\n",
    "In short, there is an additional term that incorporates the direction of the parameter update from the previous step, giving the current parameter update 'momentum' in this direction. $\\gamma$ is a user defined hyperparameter.\n",
    "\n",
    "\n",
    "### Adaptive Momentum (Adagrad):\n",
    "\n",
    "Adagrad adapts the learning rate to two properties of the training process:\n",
    "1. The scale of the gradient along directions in parameter space.\n",
    "2. The elapsed training time. \n",
    "\n",
    "As a result, rather than a scalar learning rate, we now have a vector learning rate $\\mathbf{\\eta}$. The Adagrad weight update for the $i$th parameter, at time $t$ takes the form:\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{t+1,i} = \\theta_{t,i} - \\eta_{t,i} g_{t,i} \\\\\n",
    "g_{t,i} = (\\nabla_{\\theta_{t}}C)_i\n",
    "\\end{align*}\n",
    "\n",
    "$g_{t,i}$ is just the $i$th component of the gradient vector. <br>\n",
    "We need to define two further terms. First, we introduce the diagonal matrix $G_t \\in \\mathbb{R}^{d \\times d}$, where $d$ is the dimension of parameter space. The $i$th nonzero element of $G_t$, $G_{t,ii}$ is the sum of the squares of the gradients in the direction of $\\theta_{i}$ from time $t=1$ up to time step $t$: $G_{t,ii} = \\sum_{j=1}^{t-1}g_{j,i}^{2}$. We can then use this term to scale the learning rate accordingly, yielding:\n",
    "\n",
    "$$\n",
    "\\eta_{t,i} = \\frac{\\eta_0}{\\sqrt{G_{t,ii} + \\epsilon}}\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is some small positive number to prevent division by zero errors and $\\eta_0$ is a hyperparameter.\n",
    "\n",
    "\n",
    "### RMSProp\n",
    "Observe that the adaptive learning rates of the Adagrad optimiser will converge to $0$ in the limit of long training times. This is because the denominator is simply the square root of the accumulated gradients squared from timestep $1$ to $t$, which is a monotonically increasing function.\n",
    "It is undesirable for the learning rate to tend toward $0$, as this reduces the amount of learning that can be done by the model in the latter stages of training. The RMSprop optimiser looks to solve this by normalising the initial hyperparameter $\\eta$ with the average of the squared gradients *over the past $w$ timesteps only*. This is achieved with an *exponentially weighted average* over the gradients so far.\n",
    "\n",
    "To formalise this, we start with a general form for a decaying, adaptive learning rate (we do this for a single parameter, leaving out the parameter subscript $i$ for this example):\n",
    "$$\n",
    "\\eta_t = \\frac{\\eta_0}{\\sqrt{\\alpha_t + \\epsilon}} \n",
    "$$\n",
    "Recall that for Adagrad, $\\alpha_t = \\sum_{j=1}^t g_j^2$, where $g_j$ is the gradient at the $j$ th timestep. <br>\n",
    "For RMSprop, we add a decay factor to give gradients from previous time steps lower weight compared to the most recent gradients. A mathematical statement of this concept is:\n",
    "$$\\alpha_t = \\sum_{j=1}^t \\rho^{t-j} \\cdot g_j^2$$ \n",
    "where $0<\\rho<1$. This allows us to give $\\alpha_t$ a recursive definition, namely $\\alpha_t = \\rho \\alpha_{t-1} + g_t^2$.\n",
    "\n",
    "RMSprop uses a slight variation on the formalisation above, with $\\alpha_t = \\rho \\alpha_{t-1} + (1-\\rho)g_t^2$. The scaling of $g_t^2$ by $(1-\\rho)$ allows the user to give the normalisation factor $\\alpha_t$ a long or short term memory;  $\\rho \\approx 0$ means that the normalisation term is dependent on the most recent gradients, whereas $\\rho \\approx 1$ leads to a learning rate that is influenced by gradients many time steps ago.\n",
    "\n",
    "The denominator of the RMS learning rate, being a running weighted average of the \n",
    "\n",
    "### AdaDelta\n",
    "\n",
    "The function of a user-defined learning rate $\\eta$ is to 'guess' the scale of the loss landscape. Do most valleys and hills in the landscape exist on a scale of 0.01,0.1,1.0,10,100 etc units?\n",
    "\n",
    "All other optimisation algos presented here are first-order methods - use first derivative of C w.r.t theta only. This messes up gradient units though.\n",
    "\n",
    "### Adam\n",
    "\n",
    "\n",
    "\n",
    "### Sources\n",
    "- https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/#:~:text=While%20training%20the%20deep%20learning,loss%20and%20improve%20the%20accuracy.\n",
    "- https://ruder.io/optimizing-gradient-descent/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7555c2-e105-4ceb-b8b2-cd6ce9d8cc4f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Experiments\n",
    "Comparing the optimisers above on a simple MNIST image classifier, via their learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbc473da-5d30-40f3-8b57-5a3c86023a41",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Imports\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch;torch.manual_seed(1)\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = torch.device(\"cude:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "671d4ceb-1216-4285-ac2f-85c1c6140284",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##Data processing\n",
    "train_batch_size = 64\n",
    "test_batch_size = 1000\n",
    "mean = 0.1307; std = 0.3081 #global mean and std of MNIST dataset\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "    (mean,),(std,))])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST('',train=True,\n",
    "                                     download=True,transform=transform)\n",
    "train_dataset,valid_dataset = torch.utils.data.random_split(train_dataset,[50000,10000])\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST('',train=False,\n",
    "                                          download=True,transform=transform)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           shuffle=True,\n",
    "                                           batch_size = train_batch_size)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                           shuffle=True,\n",
    "                                           batch_size=train_batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           shuffle=True,\n",
    "                                           batch_size = test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4fa839c-7cc9-4e45-8301-405c5864498d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##Create the network\n",
    "\n",
    "\n",
    "#Creating network of size [input, 128,64,32,16,10]\n",
    "\n",
    "imgs,targets = next(iter(train_loader))\n",
    "input_dims = tuple((imgs[0][0,:,:]).size())\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(input_dims[0]*input_dims[1],128)\n",
    "        self.l2 = nn.Linear(128,64)\n",
    "        self.l3 = nn.Linear(64,32)\n",
    "        self.l4 = nn.Linear(32,10)\n",
    "    \n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = x.squeeze(1).view(-1,784)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        \n",
    "        probs = F.softmax(x,dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3336aa64-baeb-42f3-bd16-38216d793ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Training with SGD\n",
      "Training complete for epoch 1.\n",
      "Train loss: 1.5592936520914897 \n",
      "Validation loss 0.5612488842693863 \n",
      "\n",
      "Training complete for epoch 2.\n",
      "Train loss: 0.4195041151150413 \n",
      "Validation loss 0.3482708727857869 \n",
      "\n",
      "Training complete for epoch 3.\n",
      "Train loss: 0.3141158416752925 \n",
      "Validation loss 0.2873607712566473 \n",
      "\n",
      "Training complete for epoch 4.\n",
      "Train loss: 0.2611422307856019 \n",
      "Validation loss 0.25200035826415773 \n",
      "\n",
      "Training complete for epoch 5.\n",
      "Train loss: 0.22266675697644348 \n",
      "Validation loss 0.21150056462568842 \n",
      "\n",
      "Training complete for epoch 6.\n",
      "Train loss: 0.19243391246899313 \n",
      "Validation loss 0.2004206870344414 \n",
      "\n",
      "Training complete for epoch 7.\n",
      "Train loss: 0.16940635678303592 \n",
      "Validation loss 0.18438528603903806 \n",
      "\n",
      "Training complete for epoch 8.\n",
      "Train loss: 0.15021829141299134 \n",
      "Validation loss 0.1615000026549693 \n",
      "\n",
      "Training complete for epoch 9.\n",
      "Train loss: 0.13391931339398103 \n",
      "Validation loss 0.14730099422536838 \n",
      "\n",
      "Training complete for epoch 10.\n",
      "Train loss: 0.12063513742640729 \n",
      "Validation loss 0.14034341473820483 \n",
      "\n",
      "Finished training for optimiser SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Test loss: 0.12850807905197142\n",
      "Training time: 83.37 seconds\n",
      "\n",
      "\n",
      " Training with SGD_momentum\n",
      "Training complete for epoch 1.\n",
      "Train loss: 0.13216144681010214 \n",
      "Validation loss 0.1291842139948895 \n",
      "\n",
      "Training complete for epoch 2.\n",
      "Train loss: 0.09646511680322706 \n",
      "Validation loss 0.11586207283992961 \n",
      "\n",
      "Training complete for epoch 3.\n",
      "Train loss: 0.07527049287172306 \n",
      "Validation loss 0.10050348884123526 \n",
      "\n",
      "Training complete for epoch 4.\n",
      "Train loss: 0.060475268537569266 \n",
      "Validation loss 0.09308619935445156 \n",
      "\n",
      "Training complete for epoch 5.\n",
      "Train loss: 0.05003721278135205 \n",
      "Validation loss 0.10709722437984814 \n",
      "\n",
      "Training complete for epoch 6.\n",
      "Train loss: 0.040676875092039155 \n",
      "Validation loss 0.09543061498137322 \n",
      "\n",
      "Training complete for epoch 7.\n",
      "Train loss: 0.033616515290935324 \n",
      "Validation loss 0.09010445288379505 \n",
      "\n",
      "Training complete for epoch 8.\n",
      "Train loss: 0.026821629254414184 \n",
      "Validation loss 0.09248713297535113 \n",
      "\n",
      "Training complete for epoch 9.\n",
      "Train loss: 0.02275131894875487 \n",
      "Validation loss 0.085281765249715 \n",
      "\n",
      "Training complete for epoch 10.\n",
      "Train loss: 0.01941900066462705 \n",
      "Validation loss 0.09875208523960867 \n",
      "\n",
      "Finished training for optimiser SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0.8\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Test loss: 0.09615195952355862\n",
      "Training time: 84.32 seconds\n",
      "\n",
      "\n",
      " Training with AdaGrad\n",
      "Training complete for epoch 1.\n",
      "Train loss: 0.10924201201239024 \n",
      "Validation loss 0.09341969173164315 \n",
      "\n",
      "Training complete for epoch 2.\n",
      "Train loss: 0.024767596579274125 \n",
      "Validation loss 0.09225300604783022 \n",
      "\n",
      "Training complete for epoch 3.\n",
      "Train loss: 0.014487159356962868 \n",
      "Validation loss 0.08463925259798813 \n",
      "\n",
      "Training complete for epoch 4.\n",
      "Train loss: 0.009921382643323203 \n",
      "Validation loss 0.0873717293871795 \n",
      "\n",
      "Training complete for epoch 5.\n",
      "Train loss: 0.007714917316180243 \n",
      "Validation loss 0.0859706483700329 \n",
      "\n",
      "Training complete for epoch 6.\n",
      "Train loss: 0.005856640146026397 \n",
      "Validation loss 0.08714813194319214 \n",
      "\n",
      "Training complete for epoch 7.\n",
      "Train loss: 0.004735410121869689 \n",
      "Validation loss 0.08925617391919824 \n",
      "\n",
      "Training complete for epoch 8.\n",
      "Train loss: 0.00398811682936257 \n",
      "Validation loss 0.08930130406808565 \n",
      "\n",
      "Training complete for epoch 9.\n",
      "Train loss: 0.0033876320483153546 \n",
      "Validation loss 0.08922932175700671 \n",
      "\n",
      "Training complete for epoch 10.\n",
      "Train loss: 0.003039930907130452 \n",
      "Validation loss 0.09119192845481715 \n",
      "\n",
      "Finished training for optimiser Adagrad (\n",
      "Parameter Group 0\n",
      "    eps: 1e-10\n",
      "    initial_accumulator_value: 0\n",
      "    lr: 0.01\n",
      "    lr_decay: 0\n",
      "    weight_decay: 0\n",
      ")\n",
      "Test loss: 0.08467558547854423\n",
      "Training time: 83.84 seconds\n",
      "\n",
      "\n",
      " Training with RMSprop\n",
      "Training complete for epoch 1.\n",
      "Train loss: 0.7837155052469513 \n",
      "Validation loss 0.4078590995661772 \n",
      "\n",
      "Training complete for epoch 2.\n",
      "Train loss: 0.27596481234821324 \n",
      "Validation loss 0.31962211102626886 \n",
      "\n",
      "Training complete for epoch 3.\n",
      "Train loss: 0.24723421480810945 \n",
      "Validation loss 0.27836175950890657 \n",
      "\n",
      "Training complete for epoch 4.\n",
      "Train loss: 0.23667658914176537 \n",
      "Validation loss 0.22152021841537564 \n",
      "\n",
      "Training complete for epoch 5.\n",
      "Train loss: 0.2305652336122187 \n",
      "Validation loss 0.3009287281804214 \n",
      "\n",
      "Training complete for epoch 6.\n",
      "Train loss: 0.24457092136737135 \n",
      "Validation loss 0.3118870591279119 \n",
      "\n",
      "Training complete for epoch 7.\n",
      "Train loss: 0.24873177998620646 \n",
      "Validation loss 0.4206761488880796 \n",
      "\n",
      "Training complete for epoch 8.\n",
      "Train loss: 0.26585845445426265 \n",
      "Validation loss 0.2639424922359977 \n",
      "\n",
      "Training complete for epoch 9.\n",
      "Train loss: 0.27662475369846845 \n",
      "Validation loss 0.41958202583015347 \n",
      "\n",
      "Training complete for epoch 10.\n",
      "Train loss: 0.27592499946984533 \n",
      "Validation loss 0.3176172844780858 \n",
      "\n",
      "Finished training for optimiser RMSprop (\n",
      "Parameter Group 0\n",
      "    alpha: 0.9\n",
      "    centered: False\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    weight_decay: 0\n",
      ")\n",
      "Test loss: 0.36086497008800505\n",
      "Training time: 80.25 seconds\n",
      "\n",
      "\n",
      " Training with Adadelta\n",
      "Training complete for epoch 1.\n",
      "Train loss: 0.15389551460804582 \n",
      "Validation loss 0.262320877284562 \n",
      "\n",
      "Training complete for epoch 2.\n",
      "Train loss: 0.12503120048566885 \n",
      "Validation loss 0.25297885750135657 \n",
      "\n",
      "Training complete for epoch 3.\n",
      "Train loss: 0.11385429176357974 \n",
      "Validation loss 0.24188146623560364 \n",
      "\n",
      "Training complete for epoch 4.\n",
      "Train loss: 0.10677080800426347 \n",
      "Validation loss 0.23236123319925847 \n",
      "\n",
      "Training complete for epoch 5.\n",
      "Train loss: 0.10199738625113222 \n",
      "Validation loss 0.25099814433745965 \n",
      "\n",
      "Training complete for epoch 6.\n",
      "Train loss: 0.09424034585219013 \n",
      "Validation loss 0.27137885610424345 \n",
      "\n",
      "Training complete for epoch 7.\n",
      "Train loss: 0.09111188473974877 \n",
      "Validation loss 0.26471793720149794 \n",
      "\n",
      "Training complete for epoch 8.\n",
      "Train loss: 0.09350051535004168 \n",
      "Validation loss 0.2955342750088548 \n",
      "\n",
      "Training complete for epoch 9.\n",
      "Train loss: 0.08596889945957767 \n",
      "Validation loss 0.32092330904949123 \n",
      "\n",
      "Training complete for epoch 10.\n",
      "Train loss: 0.08313834097882232 \n",
      "Validation loss 0.33744962051759253 \n",
      "\n",
      "Finished training for optimiser Adadelta (\n",
      "Parameter Group 0\n",
      "    eps: 1e-06\n",
      "    lr: 1.0\n",
      "    rho: 0.9\n",
      "    weight_decay: 0\n",
      ")\n",
      "Test loss: 0.3254016935825348\n",
      "Training time: 87.59 seconds\n",
      "\n",
      "\n",
      " Training with Adam\n",
      "Training complete for epoch 1.\n",
      "Train loss: 0.60325007493569 \n",
      "Validation loss 1.0113413660389603 \n",
      "\n",
      "Training complete for epoch 2.\n",
      "Train loss: 0.8263198197497736 \n",
      "Validation loss 0.8707465770517945 \n",
      "\n",
      "Training complete for epoch 3.\n",
      "Train loss: 0.8516250830477156 \n",
      "Validation loss 1.1254958269322755 \n",
      "\n",
      "Training complete for epoch 4.\n",
      "Train loss: 1.8156569316564009 \n",
      "Validation loss 1.8185824300073514 \n",
      "\n",
      "Training complete for epoch 5.\n",
      "Train loss: 1.8107341947153097 \n",
      "Validation loss 1.8223856733103467 \n",
      "\n",
      "Training complete for epoch 6.\n",
      "Train loss: 2.211627874228046 \n",
      "Validation loss 2.3023327672557468 \n",
      "\n",
      "Training complete for epoch 7.\n",
      "Train loss: 2.3025243605494192 \n",
      "Validation loss 2.302330938873777 \n",
      "\n",
      "Training complete for epoch 8.\n",
      "Train loss: 2.3025229977219914 \n",
      "Validation loss 2.30232895863284 \n",
      "\n",
      "Training complete for epoch 9.\n",
      "Train loss: 2.302521130617927 \n",
      "Validation loss 2.3023256116611943 \n",
      "\n",
      "Training complete for epoch 10.\n",
      "Train loss: 2.302573389104565 \n",
      "Validation loss 2.3023211227101124 \n",
      "\n",
      "Finished training for optimiser Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n",
      "Test loss: 2.3023220539093017\n",
      "Training time: 85.62 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "lr = 0.01\n",
    "momentum = 0.8\n",
    "\n",
    "n_epochs = 10\n",
    "objective = nn.CrossEntropyLoss()\n",
    "\n",
    "model = Net()\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "optimisers = {'SGD':optim.SGD(params=model.parameters(),lr=lr),\n",
    "             'SGD_momentum':optim.SGD(params=model.parameters(),lr=lr,momentum=momentum),\n",
    "             'AdaGrad':optim.Adagrad(params=model.parameters(),lr=lr), #normalise lr with sum of past gradients\n",
    "             'RMSprop':optim.RMSprop(params=model.parameters(),lr=lr,alpha=0.9), #normalise with weighted sum of past gradients\n",
    "             'Adadelta':optim.Adadelta(params=model.parameters(),rho=0.9), #like RMSprop, but use correct units\n",
    "              'Adam':optim.Adam(params=model.parameters(),lr=lr,betas=(0.9,0.999)) #yeah\n",
    "             }\n",
    "\n",
    "\n",
    "def optimiser_train(optimiser,n_epochs=3,plot=False):\n",
    "    '''Return optimiser loss data on MNIST dataset for model defined above.\n",
    "    We want to train for n epochs on optimiser, and record loss + time.\n",
    "    '''\n",
    "    train_loss_data = []\n",
    "    val_loss_data = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        epoch_train_loss = 0\n",
    "        epoch_val_loss = 0\n",
    "        \n",
    "        for idx,(inputs,targets) in enumerate(train_loader):\n",
    "            \n",
    "            inputs = inputs.to(device); targets = targets.to(device)\n",
    "            optimiser.zero_grad()\n",
    "            \n",
    "            model_output = model(inputs)\n",
    "            loss = objective(model_output,targets)\n",
    "            loss.backward() #compute gradients\n",
    "            optimiser.step() #update parameters\n",
    "            \n",
    "            epoch_train_loss+=loss.item()\n",
    "            \n",
    "        epoch_train_loss/=(idx+1) #normalise loss for number of iterations in batch. If not, cannot compare between train and val\n",
    "        \n",
    "        for idx,(inputs,targets) in enumerate(valid_loader):\n",
    "            inputs = inputs.to(device); targets = targets.to(device)\n",
    "            \n",
    "            with torch.no_grad(): #no need to compute backward graph as no update made\n",
    "                model_output = model(inputs)\n",
    "            loss = objective(model_output,targets)\n",
    "            \n",
    "            epoch_val_loss+=loss.item()\n",
    "            \n",
    "        epoch_val_loss/=(idx+1)\n",
    "            \n",
    "        \n",
    "        train_loss_data.append(epoch_train_loss)\n",
    "        val_loss_data.append(epoch_val_loss)\n",
    "        print('Training complete for epoch {}.\\n'\n",
    "        'Train loss: {} \\n'\n",
    "        'Validation loss {} \\n'.format(epoch+1,train_loss_data[-1],val_loss_data[-1]))\n",
    "        \n",
    "    \n",
    "     \n",
    "    end_time = time.time()\n",
    "    training_time = round((end_time-start_time),2)\n",
    "    \n",
    "    test_loss = 0\n",
    "    for idx,(inputs,targets) in enumerate(test_loader):\n",
    "        inputs = inputs.to(device); targets = targets.to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        model_output = model(inputs)\n",
    "        loss = objective(model_output,targets)\n",
    "        test_loss+=loss.item()\n",
    "    test_loss/=(idx+1)\n",
    "    \n",
    "    print('Finished training for optimiser {}'.format(optimiser))\n",
    "    print('Test loss: {}'.format(test_loss))\n",
    "    print('Training time: {} seconds'.format(training_time))\n",
    "\n",
    "        \n",
    "        \n",
    "    return train_loss_data,val_loss_data, training_time\n",
    "    \n",
    "\n",
    "fig1,ax1 = plt.subplots();\n",
    "fig2,ax2 = plt.subplots();\n",
    "fig1.suptitle('Optimiser validation loss')\n",
    "fig2.suptitle('Optimiser training times')\n",
    "\n",
    "training_times = []\n",
    "\n",
    "for name in list(optimisers.keys()):\n",
    "    \n",
    "    print('\\n\\n Training with {}'.format(name))\n",
    "    _,val_loss,training_time = optimiser_train(optimisers[name],n_epochs=n_epochs)\n",
    "    ax.plot(list(range(n_epochs)),val_loss,label=name)\n",
    "    training_times.append(training_time)\n",
    "    \n",
    "ax2.bar(list(optimisers.keys()),times)\n",
    "\n",
    "fig1.tight_layout();\n",
    "fig2.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38411fd7-04b2-45da-9a49-ae3c7fb13359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fmenv]",
   "language": "python",
   "name": "conda-env-fmenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
